{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BenchmarkNET_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ejxxOqj4Uzfp",
        "RnLGCKWPoAup",
        "kuVoRe2WoMnU",
        "AbXLkAXrfWbc"
      ],
      "mount_file_id": "1I6d7kQpz0h5q4Sbtx90TK0qUG9HD0s07",
      "authorship_tag": "ABX9TyN7SRAL26QcnR9e3b6vzesm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cengizmehmet/DesignPatterns/blob/main/BenchmarkNET_MLP/BenchmarkNET_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3WEAsv2ahop"
      },
      "source": [
        "# **AN MLP FOR BENCHMARKING**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepared by Mehmet CENGIZ**\n",
        "\n",
        "ORCID: 0000-0003-4972-167X"
      ],
      "metadata": {
        "id": "QYq8MWHIwe7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SKvgmToZwiNv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKaAOTStahlu"
      },
      "source": [
        "This particular script was created to design a deep network that can predict the performance of a given system. It is aimed to help consumers to choose computer systems that fit their needs. In order to realise this, [The Standard Performance Evaluation Corporation's](https://www.spec.org) benchmarking dataset, which was presented in 2006, was selected to train the network. It is called [SPEC 2006 Dataset](https://www.spec.org/cgi-bin/osgresults?conf=cpu2006&op=form).\n",
        "\n",
        "\n",
        "The dataset is quite messy even though practitioners gathered benchmark specifications in it carefully. In order to present a common script to clean the dataset, we created a [script](https://github.com/cengizmehmet/Databases/blob/master/SPEC2006/SPEC2006_Cleaner.ipynb) named *SPEC2006_Cleaner*. The cleaning process is explained in detail in the script. As a result, we obtained a [modified file](https://github.com/cengizmehmet/Databases/blob/master/SPEC2006/SPEC2006_modified.xlsx). In case you need the original file you can visit [here](https://github.com/cengizmehmet/Databases/blob/master/SPEC2006/SPEC2006_Original.csv)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNTuJGRnahf_"
      },
      "source": [
        "### Necessary dependencies and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh9LCXhLJ2Zk"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from enum import Enum\n",
        "from typing import Tuple\n",
        "import math\n",
        "from gc import callbacks"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dependencies:**"
      ],
      "metadata": {
        "id": "rqug2QnpTSyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dependencies of this script we used can be found in [the requirements file](https://github.com/cengizmehmet/Benchmarks/blob/master/BenchmarkNET_MLP/requirements.txt)."
      ],
      "metadata": {
        "id": "yMWMCT7fDcVO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eL9US6Dbt70"
      },
      "source": [
        "## **DATA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYggWrvqahi0"
      },
      "source": [
        "The dataset contains 22 columns. While 11 columns are string-type, 9 columns are integer-type. Except for them, there are two more columns. The columns `Result` and `Baseline` are potential target attributes and they are float-type. The problem here is that needs to be solved is to change data types to the formats that our model will accept."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset**"
      ],
      "metadata": {
        "id": "ejxxOqj4Uzfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In our case, we access the dataset from our own Google Drive folder. Thus, in your study, you have to change the path where you access the dataset.**"
      ],
      "metadata": {
        "id": "e3GS_HWNU8ub"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUGZ8N6_elfX"
      },
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/Datasets/SPEC/SPEC2006_modified.xlsx' # DO NOT FORGET TO CHANGE THE PATH"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVbQAISuelcg"
      },
      "source": [
        "dataset = pd.read_excel(path)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E2VtKMuelW8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc78761-e6e6-4400-e5ef-77c05289a3cc"
      },
      "source": [
        "print(dataset.columns)\n",
        "print(\"-----\")\n",
        "rows, columns = dataset.shape\n",
        "print(\"Columns: \" + str(columns) + \"\\n\" + \"Rows: \" + str(rows))\n",
        "print(\"-----\")\n",
        "print(dataset.dtypes)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Hardware_Vendor', 'System', 'Result', 'Baseline', '#_Cores', '#_Chips',\n",
            "       'Processor', 'Processor_MHz', '#_Threads', 'Auto_Parallelization',\n",
            "       'Base_Pointer_Size', 'Peak_Pointer_Size', '1st_Level_Cache',\n",
            "       '2nd_Level_Cache', '3rd_Level_Cache', 'Other_Cache', 'Memory',\n",
            "       'Operating_System', 'File_System', 'Compiler', 'HW_Avail', 'SW_Avail'],\n",
            "      dtype='object')\n",
            "-----\n",
            "Columns: 22\n",
            "Rows: 45341\n",
            "-----\n",
            "Hardware_Vendor          object\n",
            "System                   object\n",
            "Result                  float64\n",
            "Baseline                float64\n",
            "#_Cores                   int64\n",
            "#_Chips                   int64\n",
            "Processor                object\n",
            "Processor_MHz             int64\n",
            "#_Threads                 int64\n",
            "Auto_Parallelization     object\n",
            "Base_Pointer_Size        object\n",
            "Peak_Pointer_Size        object\n",
            "1st_Level_Cache           int64\n",
            "2nd_Level_Cache           int64\n",
            "3rd_Level_Cache           int64\n",
            "Other_Cache               int64\n",
            "Memory                    int64\n",
            "Operating_System         object\n",
            "File_System              object\n",
            "Compiler                 object\n",
            "HW_Avail                 object\n",
            "SW_Avail                 object\n",
            "dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make sure prevent wrong inputs, enumerated column names are a reasonable solution."
      ],
      "metadata": {
        "id": "yLytrOd_iPVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Column_Names(Enum):\n",
        "  result = \"Result\"\n",
        "  vendor = \"Hardware_Vendor\"\n",
        "  system = \"System\""
      ],
      "metadata": {
        "id": "rYRUxeKZiQD_"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preprocessing**"
      ],
      "metadata": {
        "id": "RnLGCKWPoAup"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuCTSQHAbt4q"
      },
      "source": [
        "As said earlier, a modified version of SPEC 2006 is going to be used. We eliminated some rows and changed some columns to make the dataset more useful for our implementation.\n",
        "\n",
        "To train our models in this script, we selected the `Result` column as the target attribute and hence we do not need the `Baseline` column anymore. The `Result` column represents the response time of the system that is run under full capacity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.drop(['Baseline'], axis = 1)"
      ],
      "metadata": {
        "id": "UiJcms1HVF5P"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method Name:** adjust_columns\n",
        "\n",
        "**Parameters:** pd.DataFrame, Column_Names, bool, bool\n",
        "\n",
        "**Return:** pd.DataFrame\n",
        "\n",
        "This function adjusts data in columns according to user needs. If:\n",
        "\n",
        "*   `normalize == True` then the function factorizes non-numeric columns and normalizes numeric values between [0, 1]. Besides, if the \"normalize\" parameter is true, the \"non_numeric\" value is insignificant.\n",
        "*   `non_numerics == True` then the function factorizes non-numerics only\n",
        "*   otherwise, the function factorizes all columns"
      ],
      "metadata": {
        "id": "ZxP-DVJMbUvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_columns(dataset: pd.DataFrame, column_name: Column_Names, normalize: bool, non_numerics: bool) -> pd.DataFrame:\n",
        "  i = 0\n",
        "  while i < len(dataset.dtypes):\n",
        "    if dataset.keys()[i] != column_name.value:\n",
        "      if normalize:\n",
        "        if dataset.dtypes[i] == 'O':\n",
        "          dataset[dataset.keys()[i]] = pd.factorize(dataset[dataset.keys()[i]])[0]\n",
        "        else:\n",
        "          dataset[dataset.keys()[i]] = (dataset[dataset.keys()[i]] - dataset[dataset.keys()[i]].min()) / (dataset[dataset.keys()[i]].max() - dataset[dataset.keys()[i]].min())\n",
        "      else:\n",
        "        if non_numerics:\n",
        "          if dataset.dtypes[i] == 'O':\n",
        "            dataset[dataset.keys()[i]] = pd.factorize(dataset[dataset.keys()[i]])[0]\n",
        "          else:\n",
        "            if dataset.keys()[i] != column_name.value:\n",
        "              dataset[dataset.keys()[i]] = pd.to_numeric(dataset[dataset.keys()[i]])[0]\n",
        "        else:\n",
        "          if dataset.keys()[i] != column_name.value:\n",
        "            dataset[dataset.keys()[i]] = pd.factorize(dataset[dataset.keys()[i]])[0]\n",
        "    i += 1\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "P--r3xAV8GwA"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Splitting**"
      ],
      "metadata": {
        "id": "kuVoRe2WoMnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Method Name:** split_data\n",
        "\n",
        "**Parameters:** pd.DataFrame, float, bool, Column_Names\n",
        "\n",
        "**Return:** list, list, list, list, list, list\n",
        "\n",
        "This function splits the dataset to test and train. The parameter that holds column names is used for defining the target column. Using the `shuffle` parameter, the function shuffles the dataset or not. As a result, this produces 6 different lists that include the parts of the dataset as below:\n",
        "\n",
        "* `train`: the training data\n",
        "* `X_train`: columns of the training data without the target column\n",
        "* `y_train`: the target column of the training data\n",
        "* `test`: the test data\n",
        "* `X_test`: columns of the test data without the target column\n",
        "* `y_test`: the target column of the test data"
      ],
      "metadata": {
        "id": "JXujDgfOcmSJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQJ3OP_3fFLn"
      },
      "source": [
        "def split_data(dataset: pd.DataFrame, data_split: float, shuffle: bool, target_column: Column_Names) -> Tuple[list, list, list, list, list, list]:  \n",
        "  target_index = dataset.columns.get_loc(target_column.value)\n",
        "  data = np.array(dataset)\n",
        "  rows, columns = data.shape\n",
        "  if shuffle:\n",
        "    np.random.shuffle(data)\n",
        "  train_size = int(data_split * rows)\n",
        "  test_size = rows - train_size\n",
        "  train = data[:train_size].T\n",
        "  y_train = train[target_index]\n",
        "  X_train = np.delete(train.T, obj = target_index, axis = 1)\n",
        "  test = data[train_size:].T\n",
        "  y_test = test[target_index]\n",
        "  X_test = np.delete(test.T, obj = target_index, axis = 1)\n",
        "  return train.T, X_train, y_train, test.T, X_test, y_test"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbXLkAXrfWbc"
      },
      "source": [
        "## **MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Definition\n",
        "\n",
        "As the `Result` column has continuous values, the problem we try to solve is a regression problem, and hence, due to the nature of regression problems, `softmax` cannot be used as the activation function. Also, `accuracy` is not good for measuring performance."
      ],
      "metadata": {
        "id": "M6-Y8u1cnmrY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKSp_mCwiSHY"
      },
      "source": [
        "**Method Name:** MLP\n",
        "\n",
        "**Parameters:** Tuple, list[int], list[Activation_Functions]\n",
        "\n",
        "**Return:** keras.Sequential\n",
        "\n",
        "This function creates an MLP model according to the input shape, number of neurons, and activation functions. The parameter `layers_neurons` holds the number of neurons of each layer. The point to be noted is that the number of layers and the number of activation functions must be equal.\n",
        "\n",
        "> Example input:\n",
        "*   `input_shape` --> Tuple --> (22, )\n",
        "*   `neurons_in_layers` --> List --> [1024, 1024, 2048]\n",
        "*   `activations` --> List --> ['relu', 'relu', 'relu']\n",
        "\n",
        "A model such as above has 22 neurons in the first layer and involves 3 hidden layers with 1024, 1024, and 2048 neurons in each layer respectively. Also, each hidden layer is activated using ReLU.\n",
        "\n",
        "Due to our case, there is no need to specify the final layer. It is fixed to have a 1 output neuron with the activation function of \"linear\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG3fNzi6fFIh"
      },
      "source": [
        "def MLP(input_shape: Tuple, neurons_in_layers: list, activations: list) -> keras.Sequential:\n",
        "  count = len(neurons_in_layers)\n",
        "  model = Sequential()\n",
        "  model.add(Dense(units = neurons_in_layers[0], activation = activations[0], input_shape = input_shape))\n",
        "  for i in range(1, count):\n",
        "    model.add(Dense(units = neurons_in_layers[i], activation = activations[i]))\n",
        "  model.add(Dense(units = 1, activation='linear'))\n",
        "  return model"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4xxrq0Rt3gv"
      },
      "source": [
        "## **IMPLEMENTATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy8zpRWvt5qF"
      },
      "source": [
        "After that point, there are a few jobs left. First, we need to prepare the dataset and then decide on the hyperparameters. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enumerated types:"
      ],
      "metadata": {
        "id": "adguIMcn8ZrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We offer some metrics as enumerated. These can be used for both loss and performance metrics."
      ],
      "metadata": {
        "id": "YT_XVORLrAus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Metrics(Enum):\n",
        "  MAE = 'mean_absolute_error'\n",
        "  MSE = 'mean_squared_error'\n",
        "  RMS = 'root_mean_squared_error'"
      ],
      "metadata": {
        "id": "uo4nYadArRZb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition, we offer some optimizers and learning rate schedules:\n",
        "\n",
        "Each optimizer has a learning rate. In case the developer forget to specify the learning rate, `Keras` defines default values as below:\n",
        "\n",
        "*   `SGD`: 0.01\n",
        "*   `RMSPROP`: 0.001\n",
        "*   `Adam`: 0.001"
      ],
      "metadata": {
        "id": "yDOVu-Mg7TFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizers(Enum):\n",
        "  Adam = \"adam\"\n",
        "  Sgd = \"sgd\"\n",
        "  Rmsprop = \"rmsprop\""
      ],
      "metadata": {
        "id": "SyYQaTI57dQp"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Learning_Rate_Schedule(Enum):\n",
        "  constant = 1\n",
        "  time_based_decay = 2\n",
        "  step_decay = 3"
      ],
      "metadata": {
        "id": "spFs1AvHTGtC"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prevent wrong inputs, activation functions that our models can accept must be enumerated."
      ],
      "metadata": {
        "id": "Ji3FY0K5lwFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation_Functions(Enum):\n",
        "  ELU = \"elu\"\n",
        "  EXPO = \"exponential\"\n",
        "  GELU = \"gelu\"\n",
        "  HARD_SIGMOID = \"hard_sigmoid\"\n",
        "  LINEAR = \"linear\"\n",
        "  RELU = \"relu\"\n",
        "  SELU = \"selu\"\n",
        "  SIGMOID = \"sigmoid\"\n",
        "  SOFTPLUS = \"softplus\"\n",
        "  SOFTSIGN = \"softsign\"\n",
        "  SWISH = \"swish\"\n",
        "  TANH = \"tanh\""
      ],
      "metadata": {
        "id": "0-EoeEqPMiIW"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY7tQdgrt9Mu"
      },
      "source": [
        "### Parameters:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset:"
      ],
      "metadata": {
        "id": "vE7ouSSaK346"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = adjust_columns(dataset, Column_Names.result, False, False)\n",
        "m, n = dataset.shape\n",
        "data_split = 0.8"
      ],
      "metadata": {
        "id": "MtX7hipLK56z"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLcF8NjT_z-5"
      },
      "source": [
        "train, X_train, y_train, test, X_test, y_test = split_data(dataset, data_split, True, Column_Names.result)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiler:"
      ],
      "metadata": {
        "id": "iDlkO2uhV0g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = Metrics.MAE\n",
        "optimizer = Optimizers.Adam\n",
        "learning_rate = 0.001\n",
        "lr_schedule = Learning_Rate_Schedule.time_based_decay\n",
        "metrics = [Metrics.MAE.value]"
      ],
      "metadata": {
        "id": "OSJVLdJqVxZ4"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitter:"
      ],
      "metadata": {
        "id": "s4s_Hds3V4ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 250\n",
        "batch_size = 10\n",
        "validation_split = 0.2\n",
        "decay_drop_rate = 0.5\n",
        "epoch_drop = 10.0\n",
        "verbose = 1"
      ],
      "metadata": {
        "id": "73yrf-LUVxUp"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model:"
      ],
      "metadata": {
        "id": "MJw32kY5K-5z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkm-XMdQ_0BP"
      },
      "source": [
        "input_shape = ((n - 1), )\n",
        "neurons_in_layers = [1024, 512, 256, 128, 64, 32 ,16]\n",
        "activations = ([Activation_Functions.RELU.value, Activation_Functions.RELU.value, Activation_Functions.RELU.value,\n",
        "                Activation_Functions.RELU.value, Activation_Functions.RELU.value, Activation_Functions.RELU.value,\n",
        "                Activation_Functions.RELU.value])"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NKU24KVxnBk"
      },
      "source": [
        "### Initializing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iffY5BJ_z3M"
      },
      "source": [
        "model = MLP(input_shape, neurons_in_layers, activations)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR2MYgdryQUE",
        "outputId": "f265905e-df17-4374-d412-2e62718bed11"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_13 (Dense)            (None, 1024)              21504     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 721,409\n",
            "Trainable params: 721,409\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compiling and running:"
      ],
      "metadata": {
        "id": "bFvvsG-pvtEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if lr_schedule.value == 1:\n",
        "  if optimizer.value == \"adam\":\n",
        "    optimizer = optimizers.Adam(learning_rate = learning_rate)\n",
        "  elif optimizer.value == \"sgd\":\n",
        "    optimizer = optimizers.SGD(learning_rate = learning_rate)\n",
        "  elif optimizer.value == \"rmsprop\":\n",
        "    optimizer = optimizers.RMSprop(learning_rate = learning_rate)\n",
        "  model.compile(\n",
        "      loss = loss.value,\n",
        "      optimizer = optimizer,\n",
        "      metrics = metrics\n",
        "      )\n",
        "  history = model.fit(\n",
        "      X_train,\n",
        "      y_train,\n",
        "      epochs = epochs,\n",
        "      batch_size = batch_size,\n",
        "      verbose = verbose,\n",
        "      validation_split = validation_split\n",
        "      )\n",
        "elif lr_schedule.value == 2:\n",
        "  model.compile(\n",
        "      loss = loss.value,\n",
        "      optimizer = optimizer.value,\n",
        "      metrics = metrics\n",
        "      )  \n",
        "  decay = learning_rate / epochs\n",
        "  def lr_time_based_decay(epoch, lr):\n",
        "    return lr * 1 / (1 + decay * epoch)\n",
        "  history = model.fit(\n",
        "      X_train,\n",
        "      y_train,\n",
        "      epochs = epochs,\n",
        "      validation_split = validation_split,\n",
        "      batch_size = batch_size,\n",
        "      callbacks = [tf.keras.callbacks.LearningRateScheduler(lr_time_based_decay, verbose = 1)],\n",
        "      )  \n",
        "elif lr_schedule.value == 3:\n",
        "  model.compile(\n",
        "      loss = loss.value,\n",
        "      optimizer = optimizer.value,\n",
        "      metrics = metrics\n",
        "      )  \n",
        "  def lr_step_decay(epoch, lr):\n",
        "    drop_rate = decay_drop_rate\n",
        "    epochs_drop = epoch_drop\n",
        "    return learning_rate * math.pow(drop_rate, math.floor(epoch/epochs_drop))\n",
        "  history = model.fit(\n",
        "      X_train,\n",
        "      y_train,\n",
        "      epochs = epochs,\n",
        "      validation_split = validation_split,\n",
        "      batch_size = batch_size,\n",
        "      callbacks = [tf.keras.callbacks.LearningRateScheduler(lr_step_decay, verbose = 1)],\n",
        "      )"
      ],
      "metadata": {
        "id": "f06-dlbGStia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8917b48e-da49-4a80-a402-7bf1fd5941c0"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
            "Epoch 1/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 260.1501 - mean_absolute_error: 260.1501 - val_loss: 214.1141 - val_mean_absolute_error: 214.1141 - lr: 0.0010\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0009999960475132613.\n",
            "Epoch 2/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 209.2130 - mean_absolute_error: 209.2130 - val_loss: 189.1825 - val_mean_absolute_error: 189.1825 - lr: 1.0000e-03\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0009999880894717933.\n",
            "Epoch 3/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 183.5093 - mean_absolute_error: 183.5093 - val_loss: 207.9121 - val_mean_absolute_error: 207.9121 - lr: 9.9999e-04\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0009999760570066191.\n",
            "Epoch 4/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 166.2152 - mean_absolute_error: 166.2152 - val_loss: 148.1534 - val_mean_absolute_error: 148.1534 - lr: 9.9998e-04\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0009999600665800896.\n",
            "Epoch 5/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 151.9290 - mean_absolute_error: 151.9290 - val_loss: 137.0754 - val_mean_absolute_error: 137.0754 - lr: 9.9996e-04\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0009999401182396999.\n",
            "Epoch 6/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 139.9419 - mean_absolute_error: 139.9419 - val_loss: 125.6914 - val_mean_absolute_error: 125.6914 - lr: 9.9994e-04\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0009999160956204154.\n",
            "Epoch 7/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 135.1439 - mean_absolute_error: 135.1439 - val_loss: 152.0181 - val_mean_absolute_error: 152.0181 - lr: 9.9992e-04\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.000999888115183189.\n",
            "Epoch 8/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 129.6189 - mean_absolute_error: 129.6189 - val_loss: 119.6046 - val_mean_absolute_error: 119.6046 - lr: 9.9989e-04\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0009998561769755123.\n",
            "Epoch 9/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 125.6033 - mean_absolute_error: 125.6033 - val_loss: 133.2741 - val_mean_absolute_error: 133.2741 - lr: 9.9986e-04\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0009998201646337465.\n",
            "Epoch 10/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 119.9739 - mean_absolute_error: 119.9739 - val_loss: 106.7433 - val_mean_absolute_error: 106.7433 - lr: 9.9982e-04\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.000999780194617444.\n",
            "Epoch 11/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 118.4481 - mean_absolute_error: 118.4481 - val_loss: 116.8813 - val_mean_absolute_error: 116.8813 - lr: 9.9978e-04\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0009997361505638955.\n",
            "Epoch 12/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 114.4552 - mean_absolute_error: 114.4552 - val_loss: 106.7757 - val_mean_absolute_error: 106.7757 - lr: 9.9974e-04\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0009996881489317208.\n",
            "Epoch 13/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 112.8449 - mean_absolute_error: 112.8449 - val_loss: 125.3042 - val_mean_absolute_error: 125.3042 - lr: 9.9969e-04\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.000999636189768409.\n",
            "Epoch 14/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 108.8749 - mean_absolute_error: 108.8749 - val_loss: 94.0444 - val_mean_absolute_error: 94.0444 - lr: 9.9964e-04\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0009995801567126443.\n",
            "Epoch 15/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 106.2932 - mean_absolute_error: 106.2932 - val_loss: 107.1775 - val_mean_absolute_error: 107.1775 - lr: 9.9958e-04\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0009995201662216483.\n",
            "Epoch 16/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 101.4535 - mean_absolute_error: 101.4535 - val_loss: 92.5202 - val_mean_absolute_error: 92.5202 - lr: 9.9952e-04\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0009994562183429066.\n",
            "Epoch 17/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 98.4184 - mean_absolute_error: 98.4184 - val_loss: 103.9894 - val_mean_absolute_error: 103.9894 - lr: 9.9946e-04\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0009993883131239052.\n",
            "Epoch 18/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 98.3805 - mean_absolute_error: 98.3805 - val_loss: 100.9705 - val_mean_absolute_error: 100.9705 - lr: 9.9939e-04\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.000999316334205188.\n",
            "Epoch 19/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 94.7698 - mean_absolute_error: 94.7698 - val_loss: 87.8979 - val_mean_absolute_error: 87.8979 - lr: 9.9932e-04\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0009992403980421105.\n",
            "Epoch 20/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 94.2348 - mean_absolute_error: 94.2348 - val_loss: 85.5743 - val_mean_absolute_error: 85.5743 - lr: 9.9924e-04\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.000999160504682156.\n",
            "Epoch 21/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 91.7588 - mean_absolute_error: 91.7588 - val_loss: 124.8309 - val_mean_absolute_error: 124.8309 - lr: 9.9916e-04\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.000999076537767263.\n",
            "Epoch 22/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 91.3882 - mean_absolute_error: 91.3882 - val_loss: 95.5110 - val_mean_absolute_error: 95.5110 - lr: 9.9908e-04\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.000998988613751388.\n",
            "Epoch 23/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 92.0295 - mean_absolute_error: 92.0295 - val_loss: 79.8614 - val_mean_absolute_error: 79.8614 - lr: 9.9899e-04\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0009988967326820123.\n",
            "Epoch 24/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 85.1846 - mean_absolute_error: 85.1846 - val_loss: 88.8870 - val_mean_absolute_error: 88.8870 - lr: 9.9890e-04\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0009988008946066151.\n",
            "Epoch 25/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 85.5563 - mean_absolute_error: 85.5563 - val_loss: 77.2398 - val_mean_absolute_error: 77.2398 - lr: 9.9880e-04\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0009987009831689952.\n",
            "Epoch 26/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 83.5035 - mean_absolute_error: 83.5035 - val_loss: 72.7041 - val_mean_absolute_error: 72.7041 - lr: 9.9870e-04\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.000998597114821243.\n",
            "Epoch 27/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 80.7146 - mean_absolute_error: 80.7146 - val_loss: 76.9886 - val_mean_absolute_error: 76.9886 - lr: 9.9860e-04\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.000998489289610837.\n",
            "Epoch 28/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 78.3203 - mean_absolute_error: 78.3203 - val_loss: 111.0544 - val_mean_absolute_error: 111.0544 - lr: 9.9849e-04\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0009983775075852537.\n",
            "Epoch 29/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 80.3204 - mean_absolute_error: 80.3204 - val_loss: 74.0215 - val_mean_absolute_error: 74.0215 - lr: 9.9838e-04\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0009982616523901502.\n",
            "Epoch 30/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 77.2809 - mean_absolute_error: 77.2809 - val_loss: 79.0088 - val_mean_absolute_error: 79.0088 - lr: 9.9826e-04\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0009981418404757527.\n",
            "Epoch 31/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 78.5829 - mean_absolute_error: 78.5829 - val_loss: 71.3850 - val_mean_absolute_error: 71.3850 - lr: 9.9814e-04\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0009980180718895354.\n",
            "Epoch 32/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 74.2176 - mean_absolute_error: 74.2176 - val_loss: 71.4496 - val_mean_absolute_error: 71.4496 - lr: 9.9802e-04\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0009978903466789728.\n",
            "Epoch 33/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 76.6751 - mean_absolute_error: 76.6751 - val_loss: 71.6802 - val_mean_absolute_error: 71.6802 - lr: 9.9789e-04\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.000997758664891538.\n",
            "Epoch 34/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 69.1972 - mean_absolute_error: 69.1972 - val_loss: 76.0945 - val_mean_absolute_error: 76.0945 - lr: 9.9776e-04\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0009976230265747032.\n",
            "Epoch 35/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 75.3285 - mean_absolute_error: 75.3285 - val_loss: 84.0568 - val_mean_absolute_error: 84.0568 - lr: 9.9762e-04\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0009974834317759403.\n",
            "Epoch 36/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 68.4690 - mean_absolute_error: 68.4690 - val_loss: 72.4897 - val_mean_absolute_error: 72.4897 - lr: 9.9748e-04\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.00099733976414416.\n",
            "Epoch 37/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 71.2946 - mean_absolute_error: 71.2946 - val_loss: 70.1549 - val_mean_absolute_error: 70.1549 - lr: 9.9734e-04\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0009971921401263236.\n",
            "Epoch 38/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 75.5817 - mean_absolute_error: 75.5817 - val_loss: 77.8749 - val_mean_absolute_error: 77.8749 - lr: 9.9719e-04\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0009970405597699006.\n",
            "Epoch 39/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 67.2241 - mean_absolute_error: 67.2241 - val_loss: 98.0380 - val_mean_absolute_error: 98.0380 - lr: 9.9704e-04\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0009968850231223598.\n",
            "Epoch 40/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 71.1800 - mean_absolute_error: 71.1800 - val_loss: 83.4321 - val_mean_absolute_error: 83.4321 - lr: 9.9689e-04\n",
            "\n",
            "Epoch 00041: LearningRateScheduler setting learning rate to 0.0009967255302311693.\n",
            "Epoch 41/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 66.2458 - mean_absolute_error: 66.2458 - val_loss: 106.7165 - val_mean_absolute_error: 106.7165 - lr: 9.9673e-04\n",
            "\n",
            "Epoch 00042: LearningRateScheduler setting learning rate to 0.0009965620811437956.\n",
            "Epoch 42/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 65.6210 - mean_absolute_error: 65.6210 - val_loss: 62.7094 - val_mean_absolute_error: 62.7094 - lr: 9.9656e-04\n",
            "\n",
            "Epoch 00043: LearningRateScheduler setting learning rate to 0.0009963946759077057.\n",
            "Epoch 43/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 62.2590 - mean_absolute_error: 62.2590 - val_loss: 67.1556 - val_mean_absolute_error: 67.1556 - lr: 9.9639e-04\n",
            "\n",
            "Epoch 00044: LearningRateScheduler setting learning rate to 0.000996223314570365.\n",
            "Epoch 44/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 65.5158 - mean_absolute_error: 65.5158 - val_loss: 69.4620 - val_mean_absolute_error: 69.4620 - lr: 9.9622e-04\n",
            "\n",
            "Epoch 00045: LearningRateScheduler setting learning rate to 0.0009960479971792384.\n",
            "Epoch 45/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 64.2560 - mean_absolute_error: 64.2560 - val_loss: 92.6532 - val_mean_absolute_error: 92.6532 - lr: 9.9605e-04\n",
            "\n",
            "Epoch 00046: LearningRateScheduler setting learning rate to 0.0009958687237817897.\n",
            "Epoch 46/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 64.1314 - mean_absolute_error: 64.1314 - val_loss: 66.9888 - val_mean_absolute_error: 66.9888 - lr: 9.9587e-04\n",
            "\n",
            "Epoch 00047: LearningRateScheduler setting learning rate to 0.0009956854944254828.\n",
            "Epoch 47/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 63.7310 - mean_absolute_error: 63.7310 - val_loss: 65.9793 - val_mean_absolute_error: 65.9793 - lr: 9.9569e-04\n",
            "\n",
            "Epoch 00048: LearningRateScheduler setting learning rate to 0.0009954983091577797.\n",
            "Epoch 48/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 63.5575 - mean_absolute_error: 63.5575 - val_loss: 78.0144 - val_mean_absolute_error: 78.0144 - lr: 9.9550e-04\n",
            "\n",
            "Epoch 00049: LearningRateScheduler setting learning rate to 0.0009953071680261427.\n",
            "Epoch 49/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 60.5761 - mean_absolute_error: 60.5761 - val_loss: 65.2368 - val_mean_absolute_error: 65.2368 - lr: 9.9531e-04\n",
            "\n",
            "Epoch 00050: LearningRateScheduler setting learning rate to 0.0009951120710780326.\n",
            "Epoch 50/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 62.0879 - mean_absolute_error: 62.0879 - val_loss: 84.3641 - val_mean_absolute_error: 84.3641 - lr: 9.9511e-04\n",
            "\n",
            "Epoch 00051: LearningRateScheduler setting learning rate to 0.0009949131347529532.\n",
            "Epoch 51/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 62.4488 - mean_absolute_error: 62.4488 - val_loss: 61.2902 - val_mean_absolute_error: 61.2902 - lr: 9.9491e-04\n",
            "\n",
            "Epoch 00052: LearningRateScheduler setting learning rate to 0.0009947102427053896.\n",
            "Epoch 52/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 60.2752 - mean_absolute_error: 60.2752 - val_loss: 67.8439 - val_mean_absolute_error: 67.8439 - lr: 9.9471e-04\n",
            "\n",
            "Epoch 00053: LearningRateScheduler setting learning rate to 0.000994503394982801.\n",
            "Epoch 53/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 60.1895 - mean_absolute_error: 60.1895 - val_loss: 58.7687 - val_mean_absolute_error: 58.7687 - lr: 9.9450e-04\n",
            "\n",
            "Epoch 00054: LearningRateScheduler setting learning rate to 0.0009942925916326445.\n",
            "Epoch 54/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 56.0736 - mean_absolute_error: 56.0736 - val_loss: 61.4388 - val_mean_absolute_error: 61.4388 - lr: 9.9429e-04\n",
            "\n",
            "Epoch 00055: LearningRateScheduler setting learning rate to 0.0009940778327023785.\n",
            "Epoch 55/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 56.1049 - mean_absolute_error: 56.1049 - val_loss: 69.8828 - val_mean_absolute_error: 69.8828 - lr: 9.9408e-04\n",
            "\n",
            "Epoch 00056: LearningRateScheduler setting learning rate to 0.0009938592346291747.\n",
            "Epoch 56/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 58.0607 - mean_absolute_error: 58.0607 - val_loss: 70.4097 - val_mean_absolute_error: 70.4097 - lr: 9.9386e-04\n",
            "\n",
            "Epoch 00057: LearningRateScheduler setting learning rate to 0.0009936366810698427.\n",
            "Epoch 57/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 55.8825 - mean_absolute_error: 55.8825 - val_loss: 60.7281 - val_mean_absolute_error: 60.7281 - lr: 9.9364e-04\n",
            "\n",
            "Epoch 00058: LearningRateScheduler setting learning rate to 0.000993410172071837.\n",
            "Epoch 58/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 57.4965 - mean_absolute_error: 57.4965 - val_loss: 60.1763 - val_mean_absolute_error: 60.1763 - lr: 9.9341e-04\n",
            "\n",
            "Epoch 00059: LearningRateScheduler setting learning rate to 0.0009931797076826116.\n",
            "Epoch 59/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 54.7345 - mean_absolute_error: 54.7345 - val_loss: 61.0989 - val_mean_absolute_error: 61.0989 - lr: 9.9318e-04\n",
            "\n",
            "Epoch 00060: LearningRateScheduler setting learning rate to 0.000992945404337475.\n",
            "Epoch 60/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 55.4876 - mean_absolute_error: 55.4876 - val_loss: 68.8064 - val_mean_absolute_error: 68.8064 - lr: 9.9295e-04\n",
            "\n",
            "Epoch 00061: LearningRateScheduler setting learning rate to 0.000992707145695094.\n",
            "Epoch 61/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 55.0749 - mean_absolute_error: 55.0749 - val_loss: 61.4230 - val_mean_absolute_error: 61.4230 - lr: 9.9271e-04\n",
            "\n",
            "Epoch 00062: LearningRateScheduler setting learning rate to 0.0009924649318029213.\n",
            "Epoch 62/250\n",
            "2902/2902 [==============================] - 10s 4ms/step - loss: 53.7284 - mean_absolute_error: 53.7284 - val_loss: 56.4538 - val_mean_absolute_error: 56.4538 - lr: 9.9246e-04\n",
            "\n",
            "Epoch 00063: LearningRateScheduler setting learning rate to 0.0009922188790948655.\n",
            "Epoch 63/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 53.4091 - mean_absolute_error: 53.4091 - val_loss: 58.5462 - val_mean_absolute_error: 58.5462 - lr: 9.9222e-04\n",
            "\n",
            "Epoch 00064: LearningRateScheduler setting learning rate to 0.0009919688712309888.\n",
            "Epoch 64/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 53.0544 - mean_absolute_error: 53.0544 - val_loss: 85.7188 - val_mean_absolute_error: 85.7188 - lr: 9.9197e-04\n",
            "\n",
            "Epoch 00065: LearningRateScheduler setting learning rate to 0.0009917150246442675.\n",
            "Epoch 65/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 59.3515 - mean_absolute_error: 59.3515 - val_loss: 62.3296 - val_mean_absolute_error: 62.3296 - lr: 9.9172e-04\n",
            "\n",
            "Epoch 00066: LearningRateScheduler setting learning rate to 0.0009914572229956932.\n",
            "Epoch 66/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 53.9562 - mean_absolute_error: 53.9562 - val_loss: 67.9723 - val_mean_absolute_error: 67.9723 - lr: 9.9146e-04\n",
            "\n",
            "Epoch 00067: LearningRateScheduler setting learning rate to 0.0009911955827173098.\n",
            "Epoch 67/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 50.3403 - mean_absolute_error: 50.3403 - val_loss: 59.9091 - val_mean_absolute_error: 59.9091 - lr: 9.9120e-04\n",
            "\n",
            "Epoch 00068: LearningRateScheduler setting learning rate to 0.000990929987471038.\n",
            "Epoch 68/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 51.6030 - mean_absolute_error: 51.6030 - val_loss: 59.2474 - val_mean_absolute_error: 59.2474 - lr: 9.9093e-04\n",
            "\n",
            "Epoch 00069: LearningRateScheduler setting learning rate to 0.0009906605536879897.\n",
            "Epoch 69/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 51.2450 - mean_absolute_error: 51.2450 - val_loss: 66.1167 - val_mean_absolute_error: 66.1167 - lr: 9.9066e-04\n",
            "\n",
            "Epoch 00070: LearningRateScheduler setting learning rate to 0.0009903871650310152.\n",
            "Epoch 70/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 53.3703 - mean_absolute_error: 53.3703 - val_loss: 61.9210 - val_mean_absolute_error: 61.9210 - lr: 9.9039e-04\n",
            "\n",
            "Epoch 00071: LearningRateScheduler setting learning rate to 0.0009901099379302934.\n",
            "Epoch 71/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 52.9963 - mean_absolute_error: 52.9963 - val_loss: 57.6755 - val_mean_absolute_error: 57.6755 - lr: 9.9011e-04\n",
            "\n",
            "Epoch 00072: LearningRateScheduler setting learning rate to 0.0009898288724318734.\n",
            "Epoch 72/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 50.1626 - mean_absolute_error: 50.1626 - val_loss: 58.1454 - val_mean_absolute_error: 58.1454 - lr: 9.8983e-04\n",
            "\n",
            "Epoch 00073: LearningRateScheduler setting learning rate to 0.0009895438521999983.\n",
            "Epoch 73/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 50.4577 - mean_absolute_error: 50.4577 - val_loss: 53.3178 - val_mean_absolute_error: 53.3178 - lr: 9.8954e-04\n",
            "\n",
            "Epoch 00074: LearningRateScheduler setting learning rate to 0.00098925499366345.\n",
            "Epoch 74/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 48.7727 - mean_absolute_error: 48.7727 - val_loss: 57.6106 - val_mean_absolute_error: 57.6106 - lr: 9.8926e-04\n",
            "\n",
            "Epoch 00075: LearningRateScheduler setting learning rate to 0.000988962296868274.\n",
            "Epoch 75/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 53.3683 - mean_absolute_error: 53.3683 - val_loss: 63.7025 - val_mean_absolute_error: 63.7025 - lr: 9.8896e-04\n",
            "\n",
            "Epoch 00076: LearningRateScheduler setting learning rate to 0.0009886656454801083.\n",
            "Epoch 76/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 51.2845 - mean_absolute_error: 51.2845 - val_loss: 55.1080 - val_mean_absolute_error: 55.1080 - lr: 9.8867e-04\n",
            "\n",
            "Epoch 00077: LearningRateScheduler setting learning rate to 0.0009883651559263356.\n",
            "Epoch 77/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 49.3409 - mean_absolute_error: 49.3409 - val_loss: 53.9377 - val_mean_absolute_error: 53.9377 - lr: 9.8837e-04\n",
            "\n",
            "Epoch 00078: LearningRateScheduler setting learning rate to 0.000988060828253.\n",
            "Epoch 78/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 48.8550 - mean_absolute_error: 48.8550 - val_loss: 53.7370 - val_mean_absolute_error: 53.7370 - lr: 9.8806e-04\n",
            "\n",
            "Epoch 00079: LearningRateScheduler setting learning rate to 0.0009877526625061443.\n",
            "Epoch 79/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 48.9729 - mean_absolute_error: 48.9729 - val_loss: 52.7643 - val_mean_absolute_error: 52.7643 - lr: 9.8775e-04\n",
            "\n",
            "Epoch 00080: LearningRateScheduler setting learning rate to 0.0009874406587318113.\n",
            "Epoch 80/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 47.4944 - mean_absolute_error: 47.4944 - val_loss: 70.3196 - val_mean_absolute_error: 70.3196 - lr: 9.8744e-04\n",
            "\n",
            "Epoch 00081: LearningRateScheduler setting learning rate to 0.0009871248169760418.\n",
            "Epoch 81/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 50.8001 - mean_absolute_error: 50.8001 - val_loss: 55.5819 - val_mean_absolute_error: 55.5819 - lr: 9.8712e-04\n",
            "\n",
            "Epoch 00082: LearningRateScheduler setting learning rate to 0.0009868051372848776.\n",
            "Epoch 82/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 46.2745 - mean_absolute_error: 46.2745 - val_loss: 53.4607 - val_mean_absolute_error: 53.4607 - lr: 9.8681e-04\n",
            "\n",
            "Epoch 00083: LearningRateScheduler setting learning rate to 0.000986481619704358.\n",
            "Epoch 83/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 50.3904 - mean_absolute_error: 50.3904 - val_loss: 53.6153 - val_mean_absolute_error: 53.6153 - lr: 9.8648e-04\n",
            "\n",
            "Epoch 00084: LearningRateScheduler setting learning rate to 0.0009861542642805228.\n",
            "Epoch 84/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 49.3312 - mean_absolute_error: 49.3312 - val_loss: 52.9519 - val_mean_absolute_error: 52.9519 - lr: 9.8615e-04\n",
            "\n",
            "Epoch 00085: LearningRateScheduler setting learning rate to 0.0009858230710594108.\n",
            "Epoch 85/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 47.0556 - mean_absolute_error: 47.0556 - val_loss: 54.7261 - val_mean_absolute_error: 54.7261 - lr: 9.8582e-04\n",
            "\n",
            "Epoch 00086: LearningRateScheduler setting learning rate to 0.0009854880400870594.\n",
            "Epoch 86/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 44.4543 - mean_absolute_error: 44.4543 - val_loss: 55.2226 - val_mean_absolute_error: 55.2226 - lr: 9.8549e-04\n",
            "\n",
            "Epoch 00087: LearningRateScheduler setting learning rate to 0.0009851491714095062.\n",
            "Epoch 87/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 46.7745 - mean_absolute_error: 46.7745 - val_loss: 52.5332 - val_mean_absolute_error: 52.5332 - lr: 9.8515e-04\n",
            "\n",
            "Epoch 00088: LearningRateScheduler setting learning rate to 0.0009848064650727875.\n",
            "Epoch 88/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 47.5954 - mean_absolute_error: 47.5954 - val_loss: 50.7810 - val_mean_absolute_error: 50.7810 - lr: 9.8481e-04\n",
            "\n",
            "Epoch 00089: LearningRateScheduler setting learning rate to 0.0009844599211229392.\n",
            "Epoch 89/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 44.5229 - mean_absolute_error: 44.5229 - val_loss: 52.2987 - val_mean_absolute_error: 52.2987 - lr: 9.8446e-04\n",
            "\n",
            "Epoch 00090: LearningRateScheduler setting learning rate to 0.000984109539605996.\n",
            "Epoch 90/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 44.3443 - mean_absolute_error: 44.3443 - val_loss: 54.5078 - val_mean_absolute_error: 54.5078 - lr: 9.8411e-04\n",
            "\n",
            "Epoch 00091: LearningRateScheduler setting learning rate to 0.0009837554369414195.\n",
            "Epoch 91/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 44.0905 - mean_absolute_error: 44.0905 - val_loss: 64.7820 - val_mean_absolute_error: 64.7820 - lr: 9.8376e-04\n",
            "\n",
            "Epoch 00092: LearningRateScheduler setting learning rate to 0.0009833974968008854.\n",
            "Epoch 92/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 44.8906 - mean_absolute_error: 44.8906 - val_loss: 55.5864 - val_mean_absolute_error: 55.5864 - lr: 9.8340e-04\n",
            "\n",
            "Epoch 00093: LearningRateScheduler setting learning rate to 0.0009830357192304262.\n",
            "Epoch 93/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 44.2770 - mean_absolute_error: 44.2770 - val_loss: 69.4087 - val_mean_absolute_error: 69.4087 - lr: 9.8304e-04\n",
            "\n",
            "Epoch 00094: LearningRateScheduler setting learning rate to 0.0009826702206481056.\n",
            "Epoch 94/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 45.4690 - mean_absolute_error: 45.4690 - val_loss: 51.8978 - val_mean_absolute_error: 51.8978 - lr: 9.8267e-04\n",
            "\n",
            "Epoch 00095: LearningRateScheduler setting learning rate to 0.0009823008847269928.\n",
            "Epoch 95/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 45.4321 - mean_absolute_error: 45.4321 - val_loss: 49.9834 - val_mean_absolute_error: 49.9834 - lr: 9.8230e-04\n",
            "\n",
            "Epoch 00096: LearningRateScheduler setting learning rate to 0.0009819277115131182.\n",
            "Epoch 96/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 45.6520 - mean_absolute_error: 45.6520 - val_loss: 52.3962 - val_mean_absolute_error: 52.3962 - lr: 9.8193e-04\n",
            "\n",
            "Epoch 00097: LearningRateScheduler setting learning rate to 0.0009815508174231475.\n",
            "Epoch 97/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 43.9055 - mean_absolute_error: 43.9055 - val_loss: 51.4575 - val_mean_absolute_error: 51.4575 - lr: 9.8155e-04\n",
            "\n",
            "Epoch 00098: LearningRateScheduler setting learning rate to 0.0009811700861315433.\n",
            "Epoch 98/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 42.4966 - mean_absolute_error: 42.4966 - val_loss: 53.5030 - val_mean_absolute_error: 53.5030 - lr: 9.8117e-04\n",
            "\n",
            "Epoch 00099: LearningRateScheduler setting learning rate to 0.0009807856340540392.\n",
            "Epoch 99/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 44.5416 - mean_absolute_error: 44.5416 - val_loss: 50.1078 - val_mean_absolute_error: 50.1078 - lr: 9.8079e-04\n",
            "\n",
            "Epoch 00100: LearningRateScheduler setting learning rate to 0.0009803973448660268.\n",
            "Epoch 100/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 41.7746 - mean_absolute_error: 41.7746 - val_loss: 57.9122 - val_mean_absolute_error: 57.9122 - lr: 9.8040e-04\n",
            "\n",
            "Epoch 00101: LearningRateScheduler setting learning rate to 0.000980005334982308.\n",
            "Epoch 101/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 41.7971 - mean_absolute_error: 41.7971 - val_loss: 60.0676 - val_mean_absolute_error: 60.0676 - lr: 9.8001e-04\n",
            "\n",
            "Epoch 00102: LearningRateScheduler setting learning rate to 0.0009796096044475129.\n",
            "Epoch 102/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 42.8756 - mean_absolute_error: 42.8756 - val_loss: 54.0792 - val_mean_absolute_error: 54.0792 - lr: 9.7961e-04\n",
            "\n",
            "Epoch 00103: LearningRateScheduler setting learning rate to 0.0009792100369384272.\n",
            "Epoch 103/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 42.9888 - mean_absolute_error: 42.9888 - val_loss: 46.9930 - val_mean_absolute_error: 46.9930 - lr: 9.7921e-04\n",
            "\n",
            "Epoch 00104: LearningRateScheduler setting learning rate to 0.0009788067488684543.\n",
            "Epoch 104/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 41.1062 - mean_absolute_error: 41.1062 - val_loss: 51.1965 - val_mean_absolute_error: 51.1965 - lr: 9.7881e-04\n",
            "\n",
            "Epoch 00105: LearningRateScheduler setting learning rate to 0.0009783997402822223.\n",
            "Epoch 105/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 41.5261 - mean_absolute_error: 41.5261 - val_loss: 51.6058 - val_mean_absolute_error: 51.6058 - lr: 9.7840e-04\n",
            "\n",
            "Epoch 00106: LearningRateScheduler setting learning rate to 0.0009779890112243583.\n",
            "Epoch 106/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 39.8253 - mean_absolute_error: 39.8253 - val_loss: 48.2583 - val_mean_absolute_error: 48.2583 - lr: 9.7799e-04\n",
            "\n",
            "Epoch 00107: LearningRateScheduler setting learning rate to 0.0009775745617394897.\n",
            "Epoch 107/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 39.5208 - mean_absolute_error: 39.5208 - val_loss: 49.4623 - val_mean_absolute_error: 49.4623 - lr: 9.7757e-04\n",
            "\n",
            "Epoch 00108: LearningRateScheduler setting learning rate to 0.000977156391872242.\n",
            "Epoch 108/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 41.2618 - mean_absolute_error: 41.2618 - val_loss: 55.5995 - val_mean_absolute_error: 55.5995 - lr: 9.7716e-04\n",
            "\n",
            "Epoch 00109: LearningRateScheduler setting learning rate to 0.0009767343853021887.\n",
            "Epoch 109/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 42.0721 - mean_absolute_error: 42.0721 - val_loss: 50.7801 - val_mean_absolute_error: 50.7801 - lr: 9.7673e-04\n",
            "\n",
            "Epoch 00110: LearningRateScheduler setting learning rate to 0.0009763086584399366.\n",
            "Epoch 110/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 37.8591 - mean_absolute_error: 37.8591 - val_loss: 58.2620 - val_mean_absolute_error: 58.2619 - lr: 9.7631e-04\n",
            "\n",
            "Epoch 00111: LearningRateScheduler setting learning rate to 0.0009758792695121711.\n",
            "Epoch 111/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 41.3844 - mean_absolute_error: 41.3844 - val_loss: 53.5409 - val_mean_absolute_error: 53.5409 - lr: 9.7588e-04\n",
            "\n",
            "Epoch 00112: LearningRateScheduler setting learning rate to 0.0009754461603809885.\n",
            "Epoch 112/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 38.5387 - mean_absolute_error: 38.5387 - val_loss: 52.7889 - val_mean_absolute_error: 52.7889 - lr: 9.7545e-04\n",
            "\n",
            "Epoch 00113: LearningRateScheduler setting learning rate to 0.0009750093310910118.\n",
            "Epoch 113/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 39.6389 - mean_absolute_error: 39.6389 - val_loss: 50.2503 - val_mean_absolute_error: 50.2503 - lr: 9.7501e-04\n",
            "\n",
            "Epoch 00114: LearningRateScheduler setting learning rate to 0.0009745688398682256.\n",
            "Epoch 114/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 41.8216 - mean_absolute_error: 41.8216 - val_loss: 58.1675 - val_mean_absolute_error: 58.1675 - lr: 9.7457e-04\n",
            "\n",
            "Epoch 00115: LearningRateScheduler setting learning rate to 0.0009741246285754224.\n",
            "Epoch 115/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 40.4739 - mean_absolute_error: 40.4739 - val_loss: 49.5801 - val_mean_absolute_error: 49.5801 - lr: 9.7412e-04\n",
            "\n",
            "Epoch 00116: LearningRateScheduler setting learning rate to 0.0009736767554381207.\n",
            "Epoch 116/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 39.3164 - mean_absolute_error: 39.3164 - val_loss: 53.0667 - val_mean_absolute_error: 53.0667 - lr: 9.7368e-04\n",
            "\n",
            "Epoch 00117: LearningRateScheduler setting learning rate to 0.0009732251623195767.\n",
            "Epoch 117/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 40.0840 - mean_absolute_error: 40.0840 - val_loss: 49.8382 - val_mean_absolute_error: 49.8382 - lr: 9.7323e-04\n",
            "\n",
            "Epoch 00118: LearningRateScheduler setting learning rate to 0.0009727699074448422.\n",
            "Epoch 118/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 37.9799 - mean_absolute_error: 37.9799 - val_loss: 48.4129 - val_mean_absolute_error: 48.4129 - lr: 9.7277e-04\n",
            "\n",
            "Epoch 00119: LearningRateScheduler setting learning rate to 0.0009723109908578372.\n",
            "Epoch 119/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 38.5301 - mean_absolute_error: 38.5301 - val_loss: 49.2829 - val_mean_absolute_error: 49.2829 - lr: 9.7231e-04\n",
            "\n",
            "Epoch 00120: LearningRateScheduler setting learning rate to 0.0009718484126024817.\n",
            "Epoch 120/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 40.8245 - mean_absolute_error: 40.8245 - val_loss: 51.9790 - val_mean_absolute_error: 51.9790 - lr: 9.7185e-04\n",
            "\n",
            "Epoch 00121: LearningRateScheduler setting learning rate to 0.0009713821727226941.\n",
            "Epoch 121/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 38.7823 - mean_absolute_error: 38.7823 - val_loss: 48.1880 - val_mean_absolute_error: 48.1880 - lr: 9.7138e-04\n",
            "\n",
            "Epoch 00122: LearningRateScheduler setting learning rate to 0.0009709122712623932.\n",
            "Epoch 122/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 36.6291 - mean_absolute_error: 36.6291 - val_loss: 44.3621 - val_mean_absolute_error: 44.3621 - lr: 9.7091e-04\n",
            "\n",
            "Epoch 00123: LearningRateScheduler setting learning rate to 0.0009704387082654961.\n",
            "Epoch 123/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 38.2054 - mean_absolute_error: 38.2054 - val_loss: 67.4024 - val_mean_absolute_error: 67.4024 - lr: 9.7044e-04\n",
            "\n",
            "Epoch 00124: LearningRateScheduler setting learning rate to 0.00096996148377592.\n",
            "Epoch 124/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 36.9228 - mean_absolute_error: 36.9228 - val_loss: 48.6489 - val_mean_absolute_error: 48.6489 - lr: 9.6996e-04\n",
            "\n",
            "Epoch 00125: LearningRateScheduler setting learning rate to 0.0009694805978375808.\n",
            "Epoch 125/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 39.5424 - mean_absolute_error: 39.5424 - val_loss: 46.3659 - val_mean_absolute_error: 46.3659 - lr: 9.6948e-04\n",
            "\n",
            "Epoch 00126: LearningRateScheduler setting learning rate to 0.0009689961086729657.\n",
            "Epoch 126/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 36.4532 - mean_absolute_error: 36.4532 - val_loss: 46.2871 - val_mean_absolute_error: 46.2871 - lr: 9.6900e-04\n",
            "\n",
            "Epoch 00127: LearningRateScheduler setting learning rate to 0.0009685079581469523.\n",
            "Epoch 127/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 36.7095 - mean_absolute_error: 36.7095 - val_loss: 56.5571 - val_mean_absolute_error: 56.5571 - lr: 9.6851e-04\n",
            "\n",
            "Epoch 00128: LearningRateScheduler setting learning rate to 0.0009680162044815615.\n",
            "Epoch 128/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 37.2406 - mean_absolute_error: 37.2406 - val_loss: 58.2171 - val_mean_absolute_error: 58.2171 - lr: 9.6802e-04\n",
            "\n",
            "Epoch 00129: LearningRateScheduler setting learning rate to 0.0009675208477200084.\n",
            "Epoch 129/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 37.5643 - mean_absolute_error: 37.5643 - val_loss: 47.3558 - val_mean_absolute_error: 47.3558 - lr: 9.6752e-04\n",
            "\n",
            "Epoch 00130: LearningRateScheduler setting learning rate to 0.0009670218879055082.\n",
            "Epoch 130/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 36.5023 - mean_absolute_error: 36.5023 - val_loss: 61.1238 - val_mean_absolute_error: 61.1238 - lr: 9.6702e-04\n",
            "\n",
            "Epoch 00131: LearningRateScheduler setting learning rate to 0.0009665193250812748.\n",
            "Epoch 131/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 38.3991 - mean_absolute_error: 38.3991 - val_loss: 44.2888 - val_mean_absolute_error: 44.2888 - lr: 9.6652e-04\n",
            "\n",
            "Epoch 00132: LearningRateScheduler setting learning rate to 0.0009660131592905221.\n",
            "Epoch 132/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 34.4665 - mean_absolute_error: 34.4665 - val_loss: 46.4613 - val_mean_absolute_error: 46.4613 - lr: 9.6601e-04\n",
            "\n",
            "Epoch 00133: LearningRateScheduler setting learning rate to 0.0009655033905764624.\n",
            "Epoch 133/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 35.5182 - mean_absolute_error: 35.5182 - val_loss: 46.5794 - val_mean_absolute_error: 46.5794 - lr: 9.6550e-04\n",
            "\n",
            "Epoch 00134: LearningRateScheduler setting learning rate to 0.0009649900189823081.\n",
            "Epoch 134/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 36.7439 - mean_absolute_error: 36.7439 - val_loss: 49.0713 - val_mean_absolute_error: 49.0713 - lr: 9.6499e-04\n",
            "\n",
            "Epoch 00135: LearningRateScheduler setting learning rate to 0.0009644730445512703.\n",
            "Epoch 135/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 35.8160 - mean_absolute_error: 35.8160 - val_loss: 52.2529 - val_mean_absolute_error: 52.2529 - lr: 9.6447e-04\n",
            "\n",
            "Epoch 00136: LearningRateScheduler setting learning rate to 0.0009639525255028059.\n",
            "Epoch 136/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 34.2160 - mean_absolute_error: 34.2160 - val_loss: 51.0707 - val_mean_absolute_error: 51.0707 - lr: 9.6395e-04\n",
            "\n",
            "Epoch 00137: LearningRateScheduler setting learning rate to 0.0009634284037034135.\n",
            "Epoch 137/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 33.9996 - mean_absolute_error: 33.9996 - val_loss: 51.0885 - val_mean_absolute_error: 51.0885 - lr: 9.6343e-04\n",
            "\n",
            "Epoch 00138: LearningRateScheduler setting learning rate to 0.0009629007373720835.\n",
            "Epoch 138/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 38.0076 - mean_absolute_error: 38.0076 - val_loss: 50.8190 - val_mean_absolute_error: 50.8190 - lr: 9.6290e-04\n",
            "\n",
            "Epoch 00139: LearningRateScheduler setting learning rate to 0.0009623695265513263.\n",
            "Epoch 139/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 34.3240 - mean_absolute_error: 34.3240 - val_loss: 46.8200 - val_mean_absolute_error: 46.8200 - lr: 9.6237e-04\n",
            "\n",
            "Epoch 00140: LearningRateScheduler setting learning rate to 0.0009618347712836528.\n",
            "Epoch 140/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 35.2309 - mean_absolute_error: 35.2309 - val_loss: 50.2410 - val_mean_absolute_error: 50.2410 - lr: 9.6183e-04\n",
            "\n",
            "Epoch 00141: LearningRateScheduler setting learning rate to 0.0009612964716115721.\n",
            "Epoch 141/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 34.7383 - mean_absolute_error: 34.7383 - val_loss: 45.9000 - val_mean_absolute_error: 45.9000 - lr: 9.6130e-04\n",
            "\n",
            "Epoch 00142: LearningRateScheduler setting learning rate to 0.0009607546275775929.\n",
            "Epoch 142/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 34.8043 - mean_absolute_error: 34.8043 - val_loss: 49.8982 - val_mean_absolute_error: 49.8982 - lr: 9.6075e-04\n",
            "\n",
            "Epoch 00143: LearningRateScheduler setting learning rate to 0.000960209239224224.\n",
            "Epoch 143/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 37.2576 - mean_absolute_error: 37.2576 - val_loss: 44.9952 - val_mean_absolute_error: 44.9952 - lr: 9.6021e-04\n",
            "\n",
            "Epoch 00144: LearningRateScheduler setting learning rate to 0.0009596603065939723.\n",
            "Epoch 144/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 35.9590 - mean_absolute_error: 35.9590 - val_loss: 46.5266 - val_mean_absolute_error: 46.5266 - lr: 9.5966e-04\n",
            "\n",
            "Epoch 00145: LearningRateScheduler setting learning rate to 0.0009591078879034977.\n",
            "Epoch 145/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 36.8903 - mean_absolute_error: 36.8903 - val_loss: 45.8257 - val_mean_absolute_error: 45.8257 - lr: 9.5911e-04\n",
            "\n",
            "Epoch 00146: LearningRateScheduler setting learning rate to 0.0009585519250206883.\n",
            "Epoch 146/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 32.2497 - mean_absolute_error: 32.2497 - val_loss: 50.0651 - val_mean_absolute_error: 50.0651 - lr: 9.5855e-04\n",
            "\n",
            "Epoch 00147: LearningRateScheduler setting learning rate to 0.0009579924761617375.\n",
            "Epoch 147/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 33.9971 - mean_absolute_error: 33.9971 - val_loss: 47.3209 - val_mean_absolute_error: 47.3209 - lr: 9.5799e-04\n",
            "\n",
            "Epoch 00148: LearningRateScheduler setting learning rate to 0.0009574294831949969.\n",
            "Epoch 148/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 35.5490 - mean_absolute_error: 35.5490 - val_loss: 46.2013 - val_mean_absolute_error: 46.2013 - lr: 9.5743e-04\n",
            "\n",
            "Epoch 00149: LearningRateScheduler setting learning rate to 0.0009568630043361936.\n",
            "Epoch 149/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 34.0182 - mean_absolute_error: 34.0182 - val_loss: 46.1137 - val_mean_absolute_error: 46.1137 - lr: 9.5686e-04\n",
            "\n",
            "Epoch 00150: LearningRateScheduler setting learning rate to 0.0009562930396271328.\n",
            "Epoch 150/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 34.0332 - mean_absolute_error: 34.0332 - val_loss: 48.7456 - val_mean_absolute_error: 48.7456 - lr: 9.5629e-04\n",
            "\n",
            "Epoch 00151: LearningRateScheduler setting learning rate to 0.0009557195891096203.\n",
            "Epoch 151/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 34.4628 - mean_absolute_error: 34.4628 - val_loss: 48.2504 - val_mean_absolute_error: 48.2504 - lr: 9.5572e-04\n",
            "\n",
            "Epoch 00152: LearningRateScheduler setting learning rate to 0.0009551427109979846.\n",
            "Epoch 152/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 33.6428 - mean_absolute_error: 33.6428 - val_loss: 45.7323 - val_mean_absolute_error: 45.7323 - lr: 9.5514e-04\n",
            "\n",
            "Epoch 00153: LearningRateScheduler setting learning rate to 0.0009545623471610402.\n",
            "Epoch 153/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 35.9451 - mean_absolute_error: 35.9451 - val_loss: 54.8480 - val_mean_absolute_error: 54.8480 - lr: 9.5456e-04\n",
            "\n",
            "Epoch 00154: LearningRateScheduler setting learning rate to 0.0009539784976405897.\n",
            "Epoch 154/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 35.4786 - mean_absolute_error: 35.4786 - val_loss: 46.1989 - val_mean_absolute_error: 46.1989 - lr: 9.5398e-04\n",
            "\n",
            "Epoch 00155: LearningRateScheduler setting learning rate to 0.0009533912206502633.\n",
            "Epoch 155/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 33.8269 - mean_absolute_error: 33.8269 - val_loss: 52.3992 - val_mean_absolute_error: 52.3992 - lr: 9.5339e-04\n",
            "\n",
            "Epoch 00156: LearningRateScheduler setting learning rate to 0.0009528004580595699.\n",
            "Epoch 156/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 33.7470 - mean_absolute_error: 33.7470 - val_loss: 44.8809 - val_mean_absolute_error: 44.8809 - lr: 9.5280e-04\n",
            "\n",
            "Epoch 00157: LearningRateScheduler setting learning rate to 0.0009522062680816732.\n",
            "Epoch 157/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 32.9408 - mean_absolute_error: 32.9408 - val_loss: 46.5717 - val_mean_absolute_error: 46.5717 - lr: 9.5221e-04\n",
            "\n",
            "Epoch 00158: LearningRateScheduler setting learning rate to 0.0009516086507576756.\n",
            "Epoch 158/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 33.8647 - mean_absolute_error: 33.8647 - val_loss: 86.3491 - val_mean_absolute_error: 86.3491 - lr: 9.5161e-04\n",
            "\n",
            "Epoch 00159: LearningRateScheduler setting learning rate to 0.0009510076061286796.\n",
            "Epoch 159/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 32.8885 - mean_absolute_error: 32.8885 - val_loss: 47.8962 - val_mean_absolute_error: 47.8962 - lr: 9.5101e-04\n",
            "\n",
            "Epoch 00160: LearningRateScheduler setting learning rate to 0.0009504031342357864.\n",
            "Epoch 160/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 36.3027 - mean_absolute_error: 36.3027 - val_loss: 45.4019 - val_mean_absolute_error: 45.4019 - lr: 9.5040e-04\n",
            "\n",
            "Epoch 00161: LearningRateScheduler setting learning rate to 0.000949795293290529.\n",
            "Epoch 161/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 31.3228 - mean_absolute_error: 31.3228 - val_loss: 48.3547 - val_mean_absolute_error: 48.3547 - lr: 9.4980e-04\n",
            "\n",
            "Epoch 00162: LearningRateScheduler setting learning rate to 0.0009491840251631105.\n",
            "Epoch 162/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 32.8616 - mean_absolute_error: 32.8616 - val_loss: 47.6750 - val_mean_absolute_error: 47.6750 - lr: 9.4918e-04\n",
            "\n",
            "Epoch 00163: LearningRateScheduler setting learning rate to 0.0009485693298946307.\n",
            "Epoch 163/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 31.7351 - mean_absolute_error: 31.7351 - val_loss: 43.6395 - val_mean_absolute_error: 43.6395 - lr: 9.4857e-04\n",
            "\n",
            "Epoch 00164: LearningRateScheduler setting learning rate to 0.0009479512656959223.\n",
            "Epoch 164/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 32.1431 - mean_absolute_error: 32.1431 - val_loss: 41.9075 - val_mean_absolute_error: 41.9075 - lr: 9.4795e-04\n",
            "\n",
            "Epoch 00165: LearningRateScheduler setting learning rate to 0.0009473298326073866.\n",
            "Epoch 165/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 32.5282 - mean_absolute_error: 32.5282 - val_loss: 43.8192 - val_mean_absolute_error: 43.8192 - lr: 9.4733e-04\n",
            "\n",
            "Epoch 00166: LearningRateScheduler setting learning rate to 0.0009467050306694229.\n",
            "Epoch 166/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 36.2559 - mean_absolute_error: 36.2559 - val_loss: 56.7983 - val_mean_absolute_error: 56.7983 - lr: 9.4671e-04\n",
            "\n",
            "Epoch 00167: LearningRateScheduler setting learning rate to 0.0009460768599224311.\n",
            "Epoch 167/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 30.1798 - mean_absolute_error: 30.1798 - val_loss: 41.7897 - val_mean_absolute_error: 41.7897 - lr: 9.4608e-04\n",
            "\n",
            "Epoch 00168: LearningRateScheduler setting learning rate to 0.0009454453204068099.\n",
            "Epoch 168/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 31.6011 - mean_absolute_error: 31.6011 - val_loss: 48.4772 - val_mean_absolute_error: 48.4772 - lr: 9.4545e-04\n",
            "\n",
            "Epoch 00169: LearningRateScheduler setting learning rate to 0.0009448104121629569.\n",
            "Epoch 169/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 31.5199 - mean_absolute_error: 31.5199 - val_loss: 43.4236 - val_mean_absolute_error: 43.4236 - lr: 9.4481e-04\n",
            "\n",
            "Epoch 00170: LearningRateScheduler setting learning rate to 0.00094417213523127.\n",
            "Epoch 170/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 30.7406 - mean_absolute_error: 30.7406 - val_loss: 43.5202 - val_mean_absolute_error: 43.5202 - lr: 9.4417e-04\n",
            "\n",
            "Epoch 00171: LearningRateScheduler setting learning rate to 0.0009435305478202523.\n",
            "Epoch 171/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 31.9196 - mean_absolute_error: 31.9196 - val_loss: 45.3044 - val_mean_absolute_error: 45.3044 - lr: 9.4353e-04\n",
            "\n",
            "Epoch 00172: LearningRateScheduler setting learning rate to 0.0009428855918017287.\n",
            "Epoch 172/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 31.0863 - mean_absolute_error: 31.0863 - val_loss: 45.9664 - val_mean_absolute_error: 45.9664 - lr: 9.4289e-04\n",
            "\n",
            "Epoch 00173: LearningRateScheduler setting learning rate to 0.0009422373253837356.\n",
            "Epoch 173/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 31.7938 - mean_absolute_error: 31.7938 - val_loss: 46.2715 - val_mean_absolute_error: 46.2715 - lr: 9.4224e-04\n",
            "\n",
            "Epoch 00174: LearningRateScheduler setting learning rate to 0.0009415857486059711.\n",
            "Epoch 174/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 33.5631 - mean_absolute_error: 33.5631 - val_loss: 42.5153 - val_mean_absolute_error: 42.5153 - lr: 9.4159e-04\n",
            "\n",
            "Epoch 00175: LearningRateScheduler setting learning rate to 0.0009409308615081314.\n",
            "Epoch 175/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 31.0722 - mean_absolute_error: 31.0722 - val_loss: 42.6514 - val_mean_absolute_error: 42.6514 - lr: 9.4093e-04\n",
            "\n",
            "Epoch 00176: LearningRateScheduler setting learning rate to 0.0009402726641299128.\n",
            "Epoch 176/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 31.0120 - mean_absolute_error: 31.0120 - val_loss: 43.4889 - val_mean_absolute_error: 43.4889 - lr: 9.4027e-04\n",
            "\n",
            "Epoch 00177: LearningRateScheduler setting learning rate to 0.0009396111565110103.\n",
            "Epoch 177/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 30.1647 - mean_absolute_error: 30.1647 - val_loss: 44.2990 - val_mean_absolute_error: 44.2990 - lr: 9.3961e-04\n",
            "\n",
            "Epoch 00178: LearningRateScheduler setting learning rate to 0.0009389463968575984.\n",
            "Epoch 178/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 30.6494 - mean_absolute_error: 30.6494 - val_loss: 44.9897 - val_mean_absolute_error: 44.9897 - lr: 9.3895e-04\n",
            "\n",
            "Epoch 00179: LearningRateScheduler setting learning rate to 0.0009382783270424265.\n",
            "Epoch 179/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 29.8866 - mean_absolute_error: 29.8866 - val_loss: 47.7321 - val_mean_absolute_error: 47.7321 - lr: 9.3828e-04\n",
            "\n",
            "Epoch 00180: LearningRateScheduler setting learning rate to 0.0009376070052712029.\n",
            "Epoch 180/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 32.3154 - mean_absolute_error: 32.3154 - val_loss: 42.6245 - val_mean_absolute_error: 42.6245 - lr: 9.3761e-04\n",
            "\n",
            "Epoch 00181: LearningRateScheduler setting learning rate to 0.0009369324315829222.\n",
            "Epoch 181/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 30.0209 - mean_absolute_error: 30.0209 - val_loss: 43.2343 - val_mean_absolute_error: 43.2343 - lr: 9.3693e-04\n",
            "\n",
            "Epoch 00182: LearningRateScheduler setting learning rate to 0.0009362546060165798.\n",
            "Epoch 182/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 30.6666 - mean_absolute_error: 30.6666 - val_loss: 42.5599 - val_mean_absolute_error: 42.5599 - lr: 9.3625e-04\n",
            "\n",
            "Epoch 00183: LearningRateScheduler setting learning rate to 0.0009355735286111695.\n",
            "Epoch 183/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 29.5913 - mean_absolute_error: 29.5913 - val_loss: 42.7031 - val_mean_absolute_error: 42.7031 - lr: 9.3557e-04\n",
            "\n",
            "Epoch 00184: LearningRateScheduler setting learning rate to 0.000934889199405685.\n",
            "Epoch 184/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 32.7656 - mean_absolute_error: 32.7656 - val_loss: 44.7250 - val_mean_absolute_error: 44.7250 - lr: 9.3489e-04\n",
            "\n",
            "Epoch 00185: LearningRateScheduler setting learning rate to 0.0009342016184391191.\n",
            "Epoch 185/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 29.3530 - mean_absolute_error: 29.3530 - val_loss: 50.2110 - val_mean_absolute_error: 50.2110 - lr: 9.3420e-04\n",
            "\n",
            "Epoch 00186: LearningRateScheduler setting learning rate to 0.0009335108439150833.\n",
            "Epoch 186/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 31.1775 - mean_absolute_error: 31.1775 - val_loss: 42.3702 - val_mean_absolute_error: 42.3702 - lr: 9.3351e-04\n",
            "\n",
            "Epoch 00187: LearningRateScheduler setting learning rate to 0.000932816817707485.\n",
            "Epoch 187/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 30.1440 - mean_absolute_error: 30.1440 - val_loss: 47.0626 - val_mean_absolute_error: 47.0626 - lr: 9.3282e-04\n",
            "\n",
            "Epoch 00188: LearningRateScheduler setting learning rate to 0.0009321195980194696.\n",
            "Epoch 188/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 29.8299 - mean_absolute_error: 29.8299 - val_loss: 42.3277 - val_mean_absolute_error: 42.3277 - lr: 9.3212e-04\n",
            "\n",
            "Epoch 00189: LearningRateScheduler setting learning rate to 0.0009314191848893298.\n",
            "Epoch 189/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 29.5517 - mean_absolute_error: 29.5517 - val_loss: 41.0702 - val_mean_absolute_error: 41.0702 - lr: 9.3142e-04\n",
            "\n",
            "Epoch 00190: LearningRateScheduler setting learning rate to 0.0009307155783553582.\n",
            "Epoch 190/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 30.1909 - mean_absolute_error: 30.1909 - val_loss: 46.1149 - val_mean_absolute_error: 46.1149 - lr: 9.3072e-04\n",
            "\n",
            "Epoch 00191: LearningRateScheduler setting learning rate to 0.0009300087784558463.\n",
            "Epoch 191/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 31.2755 - mean_absolute_error: 31.2755 - val_loss: 45.8788 - val_mean_absolute_error: 45.8788 - lr: 9.3001e-04\n",
            "\n",
            "Epoch 00192: LearningRateScheduler setting learning rate to 0.0009292987852290856.\n",
            "Epoch 192/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 30.5401 - mean_absolute_error: 30.5401 - val_loss: 40.6154 - val_mean_absolute_error: 40.6154 - lr: 9.2930e-04\n",
            "\n",
            "Epoch 00193: LearningRateScheduler setting learning rate to 0.000928585656876358.\n",
            "Epoch 193/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 29.1871 - mean_absolute_error: 29.1871 - val_loss: 43.0606 - val_mean_absolute_error: 43.0606 - lr: 9.2859e-04\n",
            "\n",
            "Epoch 00194: LearningRateScheduler setting learning rate to 0.0009278693352724969.\n",
            "Epoch 194/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 27.9397 - mean_absolute_error: 27.9397 - val_loss: 41.7427 - val_mean_absolute_error: 41.7427 - lr: 9.2787e-04\n",
            "\n",
            "Epoch 00195: LearningRateScheduler setting learning rate to 0.0009271498786183182.\n",
            "Epoch 195/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 29.3007 - mean_absolute_error: 29.3007 - val_loss: 44.8521 - val_mean_absolute_error: 44.8521 - lr: 9.2715e-04\n",
            "\n",
            "Epoch 00196: LearningRateScheduler setting learning rate to 0.0009264272869514135.\n",
            "Epoch 196/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 30.3507 - mean_absolute_error: 30.3507 - val_loss: 41.1944 - val_mean_absolute_error: 41.1944 - lr: 9.2643e-04\n",
            "\n",
            "Epoch 00197: LearningRateScheduler setting learning rate to 0.0009257015603093735.\n",
            "Epoch 197/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 28.7644 - mean_absolute_error: 28.7644 - val_loss: 43.5607 - val_mean_absolute_error: 43.5607 - lr: 9.2570e-04\n",
            "\n",
            "Epoch 00198: LearningRateScheduler setting learning rate to 0.0009249726987297877.\n",
            "Epoch 198/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 31.0499 - mean_absolute_error: 31.0499 - val_loss: 44.2865 - val_mean_absolute_error: 44.2865 - lr: 9.2497e-04\n",
            "\n",
            "Epoch 00199: LearningRateScheduler setting learning rate to 0.0009242407022502462.\n",
            "Epoch 199/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 29.1765 - mean_absolute_error: 29.1765 - val_loss: 40.9791 - val_mean_absolute_error: 40.9791 - lr: 9.2424e-04\n",
            "\n",
            "Epoch 00200: LearningRateScheduler setting learning rate to 0.0009235055709083376.\n",
            "Epoch 200/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 28.0059 - mean_absolute_error: 28.0059 - val_loss: 41.1191 - val_mean_absolute_error: 41.1191 - lr: 9.2351e-04\n",
            "\n",
            "Epoch 00201: LearningRateScheduler setting learning rate to 0.0009227673629027823.\n",
            "Epoch 201/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 27.1718 - mean_absolute_error: 27.1718 - val_loss: 40.1225 - val_mean_absolute_error: 40.1225 - lr: 9.2277e-04\n",
            "\n",
            "Epoch 00202: LearningRateScheduler setting learning rate to 0.0009220260782704703.\n",
            "Epoch 202/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 29.0484 - mean_absolute_error: 29.0484 - val_loss: 42.6308 - val_mean_absolute_error: 42.6308 - lr: 9.2203e-04\n",
            "\n",
            "Epoch 00203: LearningRateScheduler setting learning rate to 0.0009212816588876247.\n",
            "Epoch 203/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 31.8167 - mean_absolute_error: 31.8167 - val_loss: 44.9928 - val_mean_absolute_error: 44.9928 - lr: 9.2128e-04\n",
            "\n",
            "Epoch 00204: LearningRateScheduler setting learning rate to 0.0009205341629522662.\n",
            "Epoch 204/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 27.6866 - mean_absolute_error: 27.6866 - val_loss: 46.2611 - val_mean_absolute_error: 46.2611 - lr: 9.2053e-04\n",
            "\n",
            "Epoch 00205: LearningRateScheduler setting learning rate to 0.0009197835905012838.\n",
            "Epoch 205/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 29.6598 - mean_absolute_error: 29.6598 - val_loss: 41.2516 - val_mean_absolute_error: 41.2516 - lr: 9.1978e-04\n",
            "\n",
            "Epoch 00206: LearningRateScheduler setting learning rate to 0.0009190299997315347.\n",
            "Epoch 206/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 26.8777 - mean_absolute_error: 26.8777 - val_loss: 41.3024 - val_mean_absolute_error: 41.3024 - lr: 9.1903e-04\n",
            "\n",
            "Epoch 00207: LearningRateScheduler setting learning rate to 0.000918273332519472.\n",
            "Epoch 207/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 28.6989 - mean_absolute_error: 28.6989 - val_loss: 43.2004 - val_mean_absolute_error: 43.2004 - lr: 9.1827e-04\n",
            "\n",
            "Epoch 00208: LearningRateScheduler setting learning rate to 0.0009175136470614868.\n",
            "Epoch 208/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 27.9182 - mean_absolute_error: 27.9182 - val_loss: 47.3904 - val_mean_absolute_error: 47.3904 - lr: 9.1751e-04\n",
            "\n",
            "Epoch 00209: LearningRateScheduler setting learning rate to 0.0009167508852344961.\n",
            "Epoch 209/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 27.7733 - mean_absolute_error: 27.7733 - val_loss: 40.3403 - val_mean_absolute_error: 40.3403 - lr: 9.1675e-04\n",
            "\n",
            "Epoch 00210: LearningRateScheduler setting learning rate to 0.000915985105234425.\n",
            "Epoch 210/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 27.8737 - mean_absolute_error: 27.8737 - val_loss: 45.7293 - val_mean_absolute_error: 45.7293 - lr: 9.1599e-04\n",
            "\n",
            "Epoch 00211: LearningRateScheduler setting learning rate to 0.0009152163070974617.\n",
            "Epoch 211/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 27.1277 - mean_absolute_error: 27.1277 - val_loss: 45.2943 - val_mean_absolute_error: 45.2943 - lr: 9.1522e-04\n",
            "\n",
            "Epoch 00212: LearningRateScheduler setting learning rate to 0.0009144444908597927.\n",
            "Epoch 212/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 29.7390 - mean_absolute_error: 29.7390 - val_loss: 40.2023 - val_mean_absolute_error: 40.2023 - lr: 9.1444e-04\n",
            "\n",
            "Epoch 00213: LearningRateScheduler setting learning rate to 0.0009136697147159476.\n",
            "Epoch 213/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 29.1765 - mean_absolute_error: 29.1765 - val_loss: 42.4114 - val_mean_absolute_error: 42.4114 - lr: 9.1367e-04\n",
            "\n",
            "Epoch 00214: LearningRateScheduler setting learning rate to 0.0009128919205433047.\n",
            "Epoch 214/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 28.9337 - mean_absolute_error: 28.9337 - val_loss: 39.8075 - val_mean_absolute_error: 39.8075 - lr: 9.1289e-04\n",
            "\n",
            "Epoch 00215: LearningRateScheduler setting learning rate to 0.0009121111665359271.\n",
            "Epoch 215/250\n",
            "2902/2902 [==============================] - 13s 4ms/step - loss: 26.0841 - mean_absolute_error: 26.0841 - val_loss: 40.8862 - val_mean_absolute_error: 40.8862 - lr: 9.1211e-04\n",
            "\n",
            "Epoch 00216: LearningRateScheduler setting learning rate to 0.0009113274527293023.\n",
            "Epoch 216/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 27.4889 - mean_absolute_error: 27.4889 - val_loss: 94.9348 - val_mean_absolute_error: 94.9348 - lr: 9.1133e-04\n",
            "\n",
            "Epoch 00217: LearningRateScheduler setting learning rate to 0.0009105407210015044.\n",
            "Epoch 217/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 27.1019 - mean_absolute_error: 27.1019 - val_loss: 40.7186 - val_mean_absolute_error: 40.7186 - lr: 9.1054e-04\n",
            "\n",
            "Epoch 00218: LearningRateScheduler setting learning rate to 0.0009097510295458973.\n",
            "Epoch 218/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 27.3529 - mean_absolute_error: 27.3529 - val_loss: 44.4324 - val_mean_absolute_error: 44.4324 - lr: 9.0975e-04\n",
            "\n",
            "Epoch 00219: LearningRateScheduler setting learning rate to 0.0009089584365549152.\n",
            "Epoch 219/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 27.6495 - mean_absolute_error: 27.6495 - val_loss: 39.7716 - val_mean_absolute_error: 39.7716 - lr: 9.0896e-04\n",
            "\n",
            "Epoch 00220: LearningRateScheduler setting learning rate to 0.0009081628839066302.\n",
            "Epoch 220/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 28.4600 - mean_absolute_error: 28.4600 - val_loss: 46.4178 - val_mean_absolute_error: 46.4178 - lr: 9.0816e-04\n",
            "\n",
            "Epoch 00221: LearningRateScheduler setting learning rate to 0.0009073644297930102.\n",
            "Epoch 221/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 26.5952 - mean_absolute_error: 26.5952 - val_loss: 41.2282 - val_mean_absolute_error: 41.2282 - lr: 9.0736e-04\n",
            "\n",
            "Epoch 00222: LearningRateScheduler setting learning rate to 0.0009065630160925912.\n",
            "Epoch 222/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 27.7874 - mean_absolute_error: 27.7874 - val_loss: 43.2762 - val_mean_absolute_error: 43.2762 - lr: 9.0656e-04\n",
            "\n",
            "Epoch 00223: LearningRateScheduler setting learning rate to 0.0009057587009968753.\n",
            "Epoch 223/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 28.7567 - mean_absolute_error: 28.7567 - val_loss: 58.6262 - val_mean_absolute_error: 58.6262 - lr: 9.0576e-04\n",
            "\n",
            "Epoch 00224: LearningRateScheduler setting learning rate to 0.000904951484540648.\n",
            "Epoch 224/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 26.1844 - mean_absolute_error: 26.1844 - val_loss: 40.7088 - val_mean_absolute_error: 40.7088 - lr: 9.0495e-04\n",
            "\n",
            "Epoch 00225: LearningRateScheduler setting learning rate to 0.0009041413667586945.\n",
            "Epoch 225/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 26.3478 - mean_absolute_error: 26.3478 - val_loss: 41.4743 - val_mean_absolute_error: 41.4743 - lr: 9.0414e-04\n",
            "\n",
            "Epoch 00226: LearningRateScheduler setting learning rate to 0.0009033283476857995.\n",
            "Epoch 226/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 26.2712 - mean_absolute_error: 26.2712 - val_loss: 42.1134 - val_mean_absolute_error: 42.1134 - lr: 9.0333e-04\n",
            "\n",
            "Epoch 00227: LearningRateScheduler setting learning rate to 0.0009025124855118356.\n",
            "Epoch 227/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 26.3371 - mean_absolute_error: 26.3371 - val_loss: 39.1466 - val_mean_absolute_error: 39.1466 - lr: 9.0251e-04\n",
            "\n",
            "Epoch 00228: LearningRateScheduler setting learning rate to 0.000901693722116033.\n",
            "Epoch 228/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 27.2648 - mean_absolute_error: 27.2648 - val_loss: 39.9986 - val_mean_absolute_error: 39.9986 - lr: 9.0169e-04\n",
            "\n",
            "Epoch 00229: LearningRateScheduler setting learning rate to 0.000900872115687798.\n",
            "Epoch 229/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 26.4618 - mean_absolute_error: 26.4618 - val_loss: 41.6728 - val_mean_absolute_error: 41.6728 - lr: 9.0087e-04\n",
            "\n",
            "Epoch 00230: LearningRateScheduler setting learning rate to 0.0009000476662612165.\n",
            "Epoch 230/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 26.3999 - mean_absolute_error: 26.3999 - val_loss: 56.4346 - val_mean_absolute_error: 56.4346 - lr: 9.0005e-04\n",
            "\n",
            "Epoch 00231: LearningRateScheduler setting learning rate to 0.0008992203738703726.\n",
            "Epoch 231/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 26.0162 - mean_absolute_error: 26.0162 - val_loss: 41.9348 - val_mean_absolute_error: 41.9348 - lr: 8.9922e-04\n",
            "\n",
            "Epoch 00232: LearningRateScheduler setting learning rate to 0.0008983902385493507.\n",
            "Epoch 232/250\n",
            "2902/2902 [==============================] - 11s 4ms/step - loss: 25.4115 - mean_absolute_error: 25.4115 - val_loss: 39.6114 - val_mean_absolute_error: 39.6114 - lr: 8.9839e-04\n",
            "\n",
            "Epoch 00233: LearningRateScheduler setting learning rate to 0.0008975573184859284.\n",
            "Epoch 233/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 25.9273 - mean_absolute_error: 25.9273 - val_loss: 39.8732 - val_mean_absolute_error: 39.8732 - lr: 8.9756e-04\n",
            "\n",
            "Epoch 00234: LearningRateScheduler setting learning rate to 0.0008967215555600297.\n",
            "Epoch 234/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 28.1341 - mean_absolute_error: 28.1341 - val_loss: 47.9755 - val_mean_absolute_error: 47.9755 - lr: 8.9672e-04\n",
            "\n",
            "Epoch 00235: LearningRateScheduler setting learning rate to 0.0008958830079589664.\n",
            "Epoch 235/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 39.5545 - mean_absolute_error: 39.5545 - val_loss: 48.1123 - val_mean_absolute_error: 48.1123 - lr: 8.9588e-04\n",
            "\n",
            "Epoch 00236: LearningRateScheduler setting learning rate to 0.0008950416757161235.\n",
            "Epoch 236/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 28.4737 - mean_absolute_error: 28.4737 - val_loss: 40.9619 - val_mean_absolute_error: 40.9619 - lr: 8.9504e-04\n",
            "\n",
            "Epoch 00237: LearningRateScheduler setting learning rate to 0.0008941975588648848.\n",
            "Epoch 237/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 24.2088 - mean_absolute_error: 24.2088 - val_loss: 44.2899 - val_mean_absolute_error: 44.2899 - lr: 8.9420e-04\n",
            "\n",
            "Epoch 00238: LearningRateScheduler setting learning rate to 0.0008933506574386345.\n",
            "Epoch 238/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 26.3571 - mean_absolute_error: 26.3571 - val_loss: 44.3043 - val_mean_absolute_error: 44.3043 - lr: 8.9335e-04\n",
            "\n",
            "Epoch 00239: LearningRateScheduler setting learning rate to 0.0008925009714707551.\n",
            "Epoch 239/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 26.9578 - mean_absolute_error: 26.9578 - val_loss: 38.1793 - val_mean_absolute_error: 38.1793 - lr: 8.9250e-04\n",
            "\n",
            "Epoch 00240: LearningRateScheduler setting learning rate to 0.0008916485591466972.\n",
            "Epoch 240/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 26.2458 - mean_absolute_error: 26.2458 - val_loss: 40.0365 - val_mean_absolute_error: 40.0365 - lr: 8.9165e-04\n",
            "\n",
            "Epoch 00241: LearningRateScheduler setting learning rate to 0.0008907934204991453.\n",
            "Epoch 241/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 26.0001 - mean_absolute_error: 26.0001 - val_loss: 54.8016 - val_mean_absolute_error: 54.8016 - lr: 8.9079e-04\n",
            "\n",
            "Epoch 00242: LearningRateScheduler setting learning rate to 0.0008899354974091816.\n",
            "Epoch 242/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 25.1492 - mean_absolute_error: 25.1492 - val_loss: 43.5847 - val_mean_absolute_error: 43.5847 - lr: 8.8994e-04\n",
            "\n",
            "Epoch 00243: LearningRateScheduler setting learning rate to 0.0008890748480615566.\n",
            "Epoch 243/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 26.6663 - mean_absolute_error: 26.6663 - val_loss: 41.7986 - val_mean_absolute_error: 41.7986 - lr: 8.8907e-04\n",
            "\n",
            "Epoch 00244: LearningRateScheduler setting learning rate to 0.0008882115306400926.\n",
            "Epoch 244/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 26.5766 - mean_absolute_error: 26.5766 - val_loss: 44.1168 - val_mean_absolute_error: 44.1168 - lr: 8.8821e-04\n",
            "\n",
            "Epoch 00245: LearningRateScheduler setting learning rate to 0.0008873454870258688.\n",
            "Epoch 245/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 24.1828 - mean_absolute_error: 24.1828 - val_loss: 40.6133 - val_mean_absolute_error: 40.6133 - lr: 8.8735e-04\n",
            "\n",
            "Epoch 00246: LearningRateScheduler setting learning rate to 0.0008864767172515682.\n",
            "Epoch 246/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 25.8567 - mean_absolute_error: 25.8567 - val_loss: 38.6939 - val_mean_absolute_error: 38.6939 - lr: 8.8648e-04\n",
            "\n",
            "Epoch 00247: LearningRateScheduler setting learning rate to 0.0008856052795003128.\n",
            "Epoch 247/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 24.0492 - mean_absolute_error: 24.0492 - val_loss: 41.7785 - val_mean_absolute_error: 41.7785 - lr: 8.8561e-04\n",
            "\n",
            "Epoch 00248: LearningRateScheduler setting learning rate to 0.0008847311738040873.\n",
            "Epoch 248/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 27.1347 - mean_absolute_error: 27.1347 - val_loss: 42.3210 - val_mean_absolute_error: 42.3210 - lr: 8.8473e-04\n",
            "\n",
            "Epoch 00249: LearningRateScheduler setting learning rate to 0.0008838544001948747.\n",
            "Epoch 249/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 24.8627 - mean_absolute_error: 24.8627 - val_loss: 37.8103 - val_mean_absolute_error: 37.8103 - lr: 8.8385e-04\n",
            "\n",
            "Epoch 00250: LearningRateScheduler setting learning rate to 0.0008829749587046588.\n",
            "Epoch 250/250\n",
            "2902/2902 [==============================] - 12s 4ms/step - loss: 24.1779 - mean_absolute_error: 24.1779 - val_loss: 39.3106 - val_mean_absolute_error: 39.3106 - lr: 8.8297e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RESULTS**"
      ],
      "metadata": {
        "id": "X57kf0Kre7Hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training results"
      ],
      "metadata": {
        "id": "spLnwQa5fVt-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC0XJkwvzDE2"
      },
      "source": [
        "We parse the `history` dictionary into `loss` and `validation loss`. By using them, we calculate the average losses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pnVhCQMy-FJ"
      },
      "source": [
        "loss_values = history.history['loss']\n",
        "val_loss_values = history.history['val_loss']\n",
        "metric = history.history['mean_absolute_error']\n",
        "if lr_schedule.value != 1:\n",
        "  lr = history.history['lr']\n",
        "avg_loss = sum(loss_values)/len(loss_values)\n",
        "avg_val_loss = sum(val_loss_values)/len(val_loss_values)\n",
        "avg_metric = sum(metric)/len(metric)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(avg_loss)\n",
        "print(avg_val_loss)\n",
        "print(avg_metric)\n",
        "if lr_schedule.value != 1:\n",
        "  print(min(lr))"
      ],
      "metadata": {
        "id": "cyw0SM9DWbYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6be14fec-b5bc-4c8e-95cc-db1427ba393e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48.957771026611326\n",
            "59.77759461975098\n",
            "48.95777113342285\n",
            "0.00088297494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGNsuO7VyycD"
      },
      "source": [
        "#### Visualisation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss and validation loss:"
      ],
      "metadata": {
        "id": "xqQTRU-94OVd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlTlv80Gy-IE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "b213aeba-7544-4b42-ac14-8317145def01"
      },
      "source": [
        "epochs = range(1, len(loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'r', label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning rate:"
      ],
      "metadata": {
        "id": "QyNzNhHf4Sfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if lr_schedule.value != 1:\n",
        "  plt.plot(epochs, lr, 'b', label='Learning Rate')\n",
        "  plt.yscale('log')"
      ],
      "metadata": {
        "id": "uNiIYEfh4SMW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "2d6501b1-6680-4c2b-ed5b-19344eb2ea1f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWVfKT5oBYAW"
      },
      "source": [
        "### Test results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To measure the successes of tests, we decided to use `R squared` metric."
      ],
      "metadata": {
        "id": "gDMbccVqgzrS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdk9dcp_Eb3L"
      },
      "source": [
        "y_predicted = model.predict(X_test)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcbY5ouKB67M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96660f17-0144-48c9-c68c-cf8b4aad4800"
      },
      "source": [
        "testR2Value = r2_score(y_test,y_predicted)\n",
        "print(\"Training Set R-Square=\", testR2Value)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set R-Square= 0.922974953698734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualisation\n",
        "\n",
        "And below, we present the distribution of the predicted values over true values for the MLP model."
      ],
      "metadata": {
        "id": "c3f05JclbBAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(y_test, y_predicted, c='crimson')\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "p1 = max(max(y_predicted), max(y_test))\n",
        "p2 = min(min(y_predicted), min(y_test))\n",
        "plt.plot([p1, p2], [p1, p2], 'b-')\n",
        "plt.xlabel('True Values', fontsize=15)\n",
        "plt.ylabel('Predictions', fontsize=15)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WP52yc_Nh7CX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "outputId": "16847fc0-7f29-4e00-f626-f93185c0b4ea"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order, subok=True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}